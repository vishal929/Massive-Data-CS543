{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd4a235-3cf1-4c99-9ae7-13da5d79ad7e",
   "metadata": {},
   "source": [
    "**Processing airline data Group 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9ee1ae-ee08-4f1d-8408-ea33c4b5f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n",
      "[Row(Year=1993, Month=1, DayofMonth=29, DayOfWeek=5, DepTime='1055', CRSDepTime=1055, ArrTime='1228', CRSArrTime=1212, UniqueCarrier='US', FlightNum=66, TailNum='NA', ActualElapsedTime='93', CRSElapsedTime='77', AirTime='NA', ArrDelay='16', DepDelay='0', Origin='EWR', Dest='BUF', Distance='282', TaxiIn='NA', TaxiOut='NA', Cancelled=0, CancellationCode='NA', Diverted=0, CarrierDelay='NA', WeatherDelay='NA', NASDelay='NA', SecurityDelay='NA', LateAircraftDelay='NA')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\",60)\n",
    "#spark.conf.set(\"spark.default.parallelism\",60)\n",
    "#spark.conf.set(\"spark.local.dir\",'./spark_tmp')\n",
    "# grabbing all csvs in raw folder just because\n",
    "raw = spark.read.options(header=True,inferSchema=True).csv('airplane_raw')\n",
    "raw.printSchema()\n",
    "print(raw.take(1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcac41-a598-44c1-b38f-ab35a9dc3b15",
   "metadata": {},
   "source": [
    "**How many unique airports are in our data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f173127-4dd7-4ab9-951f-bbd3ebad7d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+-----+-------+-----------+------------+\n",
      "|iata|             airport|              city|state|country|        lat|        long|\n",
      "+----+--------------------+------------------+-----+-------+-----------+------------+\n",
      "| 00M|            Thigpen |       Bay Springs|   MS|    USA|31.95376472|-89.23450472|\n",
      "| 00R|Livingston Municipal|        Livingston|   TX|    USA|30.68586111|-95.01792778|\n",
      "| 00V|         Meadow Lake|  Colorado Springs|   CO|    USA|38.94574889|-104.5698933|\n",
      "| 01G|        Perry-Warsaw|             Perry|   NY|    USA|42.74134667|-78.05208056|\n",
      "| 01J|    Hilliard Airpark|          Hilliard|   FL|    USA| 30.6880125|-81.90594389|\n",
      "| 01M|   Tishomingo County|           Belmont|   MS|    USA|34.49166667|-88.20111111|\n",
      "| 02A|         Gragg-Wade |           Clanton|   AL|    USA|32.85048667|-86.61145333|\n",
      "| 02C|             Capitol|        Brookfield|   WI|    USA|   43.08751|-88.17786917|\n",
      "| 02G|   Columbiana County|    East Liverpool|   OH|    USA|40.67331278|-80.64140639|\n",
      "| 03D|    Memphis Memorial|           Memphis|   MO|    USA|40.44725889|-92.22696056|\n",
      "| 04M|      Calhoun County|         Pittsboro|   MS|    USA|33.93011222|-89.34285194|\n",
      "| 04Y|    Hawley Municipal|            Hawley|   MN|    USA|46.88384889|-96.35089861|\n",
      "| 05C|Griffith-Merrillv...|          Griffith|   IN|    USA|41.51961917|-87.40109333|\n",
      "| 05F|Gatesville - City...|        Gatesville|   TX|    USA|31.42127556|-97.79696778|\n",
      "| 05U|              Eureka|            Eureka|   NV|    USA|39.60416667|-116.0050597|\n",
      "| 06A|    Moton  Municipal|          Tuskegee|   AL|    USA|32.46047167|-85.68003611|\n",
      "| 06C|          Schaumburg|Chicago/Schaumburg|   IL|    USA|41.98934083|-88.10124278|\n",
      "| 06D|     Rolla Municipal|             Rolla|   ND|    USA|48.88434111|-99.62087694|\n",
      "| 06M|    Eupora Municipal|            Eupora|   MS|    USA|33.53456583|-89.31256917|\n",
      "| 06N|            Randall |        Middletown|   NY|    USA|41.43156583|-74.39191722|\n",
      "+----+--------------------+------------------+-----+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|Origin|\n",
      "+------+\n",
      "|   BGM|\n",
      "|   DLG|\n",
      "|   PSE|\n",
      "|   MSY|\n",
      "|   GEG|\n",
      "|   BUR|\n",
      "|   SNA|\n",
      "|   GRB|\n",
      "|   GTF|\n",
      "|   IDA|\n",
      "|   GRR|\n",
      "|   EUG|\n",
      "|   PSG|\n",
      "|   GSO|\n",
      "|   PVD|\n",
      "|   MYR|\n",
      "|   OAK|\n",
      "|   MSN|\n",
      "|   FAR|\n",
      "|   BTM|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_airports = spark.read.options(header=True,inferSchema=True).csv('airplane_raw/metadata/airports.csv')\n",
    "#print(unique_airports.count())\n",
    "unique_airports.show()\n",
    "\n",
    "actual_unique_airports = raw.select('Origin').union(raw.select('Dest')).distinct()\n",
    "actual_unique_airports.show()\n",
    "#print(actual_unique_airports.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c21f65-dc4c-4ab4-92d0-2b5f6bf26a3b",
   "metadata": {},
   "source": [
    "**How many records are flights that were cancelled vs those that were not cancelled?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83a3419-26e7-4529-811f-5c00980c01c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncancelled_count = raw.select(\\'Cancelled\\').where(\\'Cancelled = 1\\').count()\\nnon_cancelled_count = raw.select(\\'Cancelled\\').where(\\'Cancelled= 0\\').count()\\ndelayed_count = raw.select(\\'Cancelled\\',\\'DepDelay\\',\\'ArrDelay\\').where(\\'Cancelled == 1 OR DepDelay > 0 OR ArrDelay>0\\').count()\\ntotal_count = raw.count()\\nprint(\"total flights: \" + str(total_count))\\nprint(\"cancelled flights: \" + str(cancelled_count))\\nprint(\"non-cancelled flights: \" + str(non_cancelled_count))\\nprint(\"expected non_cancelled: \" + str(total_count - cancelled_count))\\nprint(\"total cancelled and delayed flights: \" + str(delayed_count))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simply counting total number and number of cancelled flights, the non-cancelled flights is the difference of these\n",
    "'''\n",
    "cancelled_count = raw.select('Cancelled').where('Cancelled = 1').count()\n",
    "non_cancelled_count = raw.select('Cancelled').where('Cancelled= 0').count()\n",
    "delayed_count = raw.select('Cancelled','DepDelay','ArrDelay').where('Cancelled == 1 OR DepDelay > 0 OR ArrDelay>0').count()\n",
    "total_count = raw.count()\n",
    "print(\"total flights: \" + str(total_count))\n",
    "print(\"cancelled flights: \" + str(cancelled_count))\n",
    "print(\"non-cancelled flights: \" + str(non_cancelled_count))\n",
    "print(\"expected non_cancelled: \" + str(total_count - cancelled_count))\n",
    "print(\"total cancelled and delayed flights: \" + str(delayed_count))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bef94-ce58-47e1-8c40-12f64dc56749",
   "metadata": {},
   "source": [
    "**Adding in weather data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c965a4-2abe-45ba-bda7-3494321b268f",
   "metadata": {},
   "source": [
    "**Firstly, getting weather stations and longitude and latitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5acf44e6-0557-4b19-bad8-715b6abf5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+---------+\n",
      "|  stationID|latitude|longitude|elevation|\n",
      "+-----------+--------+---------+---------+\n",
      "|ACW00011604| 17.1167| -61.7833|     10.1|\n",
      "|ACW00011647| 17.1333| -61.7833|     19.2|\n",
      "|AE000041196|  25.333|   55.517|     34.0|\n",
      "|AEM00041194|  25.255|   55.364|     10.4|\n",
      "|AEM00041217|  24.433|   54.651|     26.8|\n",
      "|AEM00041218|  24.262|   55.609|    264.9|\n",
      "|AF000040930|  35.317|   69.017|   3366.0|\n",
      "|AFM00040938|   34.21|   62.228|    977.2|\n",
      "|AFM00040948|  34.566|   69.212|   1791.3|\n",
      "|AFM00040990|    31.5|    65.85|   1010.0|\n",
      "|AG000060390| 36.7167|     3.25|     24.0|\n",
      "|AG000060590| 30.5667|   2.8667|    397.0|\n",
      "|AG000060611|   28.05|   9.6331|    561.0|\n",
      "|AG000060680|    22.8|   5.4331|   1362.0|\n",
      "|AGE00135039| 35.7297|     0.65|     50.0|\n",
      "|AGE00147704|   36.97|     7.79|    161.0|\n",
      "|AGE00147705|   36.78|     3.07|     59.0|\n",
      "|AGE00147706|    36.8|     3.03|    344.0|\n",
      "|AGE00147707|    36.8|     3.04|     38.0|\n",
      "|AGE00147708|   36.72|     4.05|    222.0|\n",
      "+-----------+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IV. FORMAT OF \"ghcnd-stations.txt\"\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "\n",
    "'''\n",
    "# we only want ID, latitude, longitude, and elevation, the other features do not matter to us\n",
    "weather_stations = spark.read.options().text('weather_raw/metadata/ghcnd-stations.txt').rdd\\\n",
    "                                        .map(lambda x: x['value'])\\\n",
    "                                        .map(lambda x: [x[0:11],float(x[12:20]),float(x[21:30]),float(x[31:37])])\\\n",
    "                                        .toDF(['stationID','latitude','longitude','elevation'])\n",
    "                                        \n",
    "print(weather_stations.show())\n",
    "#print(weather_stations.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcec38b-4ede-4645-a0a6-92674c8423b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching airports used in dataset to their nearest weather station\n",
    "actual_unique_airports_collect = actual_unique_airports.withColumnRenamed('Origin','iata')\\\n",
    "                                                        .join(unique_airports.select('iata','lat','long'),'iata')\n",
    "\n",
    "# dont need elevation for haversine formula\n",
    "weather_stations_collect = weather_stations.drop('elevation')\n",
    "#print(weather_stations_collect.rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2eb5d1-79b5-451d-a6ba-29a96fb7b850",
   "metadata": {},
   "source": [
    "**Matching airports to their closest weather station (so that we can fill records with relevant weather data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839450ff-7129-4127-b744-e793b06f7927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle for airport to station mapping already generated!\n"
     ]
    }
   ],
   "source": [
    "from math import radians, asin, sin,cos, sqrt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "import heapq\n",
    "import pickle\n",
    "import os\n",
    "# https://en.wikipedia.org/wiki/Haversine_formula\n",
    "# can use haversine formula to compute distance based on longitude and latitude, assumes the Earth is a perfect sphere\n",
    "\n",
    "# we want to map the airports used in our dataset to their closest weather stations, so that we can leverage weather data\n",
    "# this haversine formula will return approximate distance in km between two points with longitude and latitude\n",
    "def haversine_formula(long1,long2,lat1,lat2):\n",
    "    long1 = radians(long1)\n",
    "    long2 = radians(long2)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "    # radius of the earth as measured not from the poles\n",
    "    radius = 6356.752\n",
    "    return 2 * radius * asin( sqrt(\\\n",
    "                                       (sin((lat2-lat1)/2)**2) + \\\n",
    "                                       cos(lat1) * cos(lat2) * \\\n",
    "                                       (sin((long2-long1)/2)**2)\\\n",
    "                                  ) )\n",
    "def filter_weather_stations(record):\n",
    "    station_name = record[0]\n",
    "    if not os.path.exists('./ghcnd_all/'+station_name+'.dly'):\n",
    "        return False\n",
    "    # getting the first line of the textfile and checking if the year <=1987\n",
    "    with open('./ghcnd_all/'+station_name+'.dly') as f:\n",
    "        first_line = f.readline()\n",
    "        if int(first_line[11:15])<=1987:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# checking if we already created a pickle, if so, we do not need to do this costly computation\n",
    "if os.path.exists('./airport_closest_stations.pickle'):\n",
    "    print('pickle for airport to station mapping already generated!')\n",
    "    with open('./airport_closest_stations.pickle', 'rb') as f:\n",
    "            airport_nearest_weather_station = pickle.load(f)\n",
    "    actual_unique_airports_collect.unpersist()\n",
    "    weather_stations_collect.unpersist()\n",
    "    del(actual_unique_airports_collect)\n",
    "    del(weather_stations_collect)\n",
    "    unique_airports.unpersist()\n",
    "    del(unique_airports)\n",
    "    actual_unique_airports.unpersist()\n",
    "    del(actual_unique_airports)\n",
    "else:\n",
    "    actual_unique_airports_collect = actual_unique_airports_collect.collect()\n",
    "    #weather_stations_collect = weather_stations_collect.filter(udf(lambda record: filter_weather_stations(record), BooleanType()))\n",
    "    weather_stations_collect = weather_stations_collect.rdd\\\n",
    "                                                        .map(lambda x: (x['stationID'],x['latitude'],x['longitude']))\\\n",
    "                                                        .filter(filter_weather_stations).collect()\n",
    "    # filtering out weather stations which havent been recording since 1987\n",
    "    '''\n",
    "    final_weather_stations = []\n",
    "    count = 0\n",
    "    for weather_station_record in weather_stations_collect:\n",
    "        if os.path.exists('./ghcnd_all/'+weather_station_record[0]+'.dly'):\n",
    "            data = spark.read.options().text('./ghcnd_all/'+weather_station_record[0]+'.dly')\n",
    "            if data.rdd.map(lambda x:x['value']).map(lambda x: int(x[11:15])).sortBy(lambda x: x).take(1)[0] <= 1987:\n",
    "                # lets add it to the final list\n",
    "                final_weather_stations.append(weather_station_record)\n",
    "        count+=1\n",
    "        if (count%10000 ==0):\n",
    "            print(count)\n",
    "\n",
    "    del(weather_stations_collect)\n",
    "    weather_stations_collect = final_weather_stations\n",
    "    '''\n",
    "    # creating a map from airport to the 5 nearest weather stations\n",
    "    # if the size of max heap is 5, and the root is greater than the item in question, we can pop and insert the new item\n",
    "    get_closest = 5\n",
    "    airport_nearest_weather_station = {}\n",
    "    for airport_record in actual_unique_airports_collect:\n",
    "        max_heap=[]\n",
    "        min_dist=None\n",
    "        for weather_station_record in weather_stations_collect:\n",
    "            long1 = airport_record[2]\n",
    "            long2 = weather_station_record[2]\n",
    "            lat1 = airport_record[1]\n",
    "            lat2 = weather_station_record[1]\n",
    "            dist = haversine_formula(long1,long2,lat1,lat2)\n",
    "            if min_dist is None:\n",
    "                min_dist = dist\n",
    "            else:\n",
    "                min_dist = min(min_dist,dist)\n",
    "            if len(max_heap)<get_closest or abs(max_heap[0][0])>dist:\n",
    "                # checking if the data starts from 1981\n",
    "                #data = spark.read.options().text('./ghcnd_all/'+weather_station_record[0]+'.dly')\n",
    "                #if (data.rdd.map(lambda x:x['value']).map(lambda x: int(x[11:15])).sortBy(lambda x: x).take(1)[0] <= 1982):\n",
    "                # we are good then, lets add this station\n",
    "                # pushing negative for max heap instead of min\n",
    "                heapq.heappush(max_heap,(-min_dist,weather_station_record[0]))\n",
    "                if len(max_heap)>get_closest:\n",
    "                    heapq.heappop(max_heap)\n",
    "        # inserting into map\n",
    "        airport_nearest_weather_station[airport_record[0]] = [heap_element[1] for heap_element in max_heap]\n",
    "        #print(\"matched airport to stations with min distance: \" + str(min_dist)+\" km\")\n",
    "        if min_dist > 10:\n",
    "            print('got airport: ' + airport_record[0] + ' matched to min_distance ' + str(min_dist))\n",
    "\n",
    "\n",
    "    # deleting collected arrays, we dont need them anymore\n",
    "    del(weather_stations_collect)\n",
    "    del(actual_unique_airports_collect)\n",
    "\n",
    "    # saving the results of this to file, since it took a while to generate\n",
    "    with open('./airport_closest_stations.pickle', 'wb') as f:\n",
    "            pickle.dump(airport_nearest_weather_station, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca51bc81-7cae-41e2-a1a0-795f777db08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank cell\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Each \".dly\" file contains data for one station.  The name of the file\n",
    "corresponds to a station's identification code.  For example, \"USC00026481.dly\"\n",
    "contains the data for the station with the identification code USC00026481).\n",
    "\n",
    "Each record in a file contains one month of daily data.  The variables on each\n",
    "line include the following:\n",
    "\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "YEAR         12-15   Integer\n",
    "MONTH        16-17   Integer\n",
    "ELEMENT      18-21   Character\n",
    "VALUE1       22-26   Integer\n",
    "MFLAG1       27-27   Character\n",
    "QFLAG1       28-28   Character\n",
    "SFLAG1       29-29   Character\n",
    "VALUE2       30-34   Integer\n",
    "MFLAG2       35-35   Character\n",
    "QFLAG2       36-36   Character\n",
    "SFLAG2       37-37   Character\n",
    "  .           .          .\n",
    "  .           .          .\n",
    "  .           .          .\n",
    "VALUE31    262-266   Integer\n",
    "MFLAG31    267-267   Character\n",
    "QFLAG31    268-268   Character\n",
    "SFLAG31    269-269   Character\n",
    "------------------------------\n",
    "\n",
    "These variables have the following definitions:\n",
    "\n",
    "ID         is the station identification code.  Please see \"ghcnd-stations.txt\"\n",
    "           for a complete list of stations and their metadata.\n",
    "YEAR       is the year of the record.\n",
    "\n",
    "MONTH      is the month of the record.\n",
    "\n",
    "ELEMENT    is the element type.   There are five core elements as well as a number\n",
    "           of addition elements.  \n",
    "\t   \n",
    "\t   The five core elements are:\n",
    "\n",
    "        PRCP = Precipitation (tenths of mm)\n",
    "   \t   SNOW = Snowfall (mm)\n",
    "\t   SNWD = Snow depth (mm)\n",
    "           TMAX = Maximum temperature (tenths of degrees C)\n",
    "           TMIN = Minimum temperature (tenths of degrees C)\n",
    "\t   \n",
    "\t   The other elements are:\n",
    "\t   \n",
    "\t   ACMC = Average cloudiness midnight to midnight from 30-second \n",
    "\t          ceilometer data (percent)\n",
    "\t   ACMH = Average cloudiness midnight to midnight from \n",
    "\t          manual observations (percent)\n",
    "           ACSC = Average cloudiness sunrise to sunset from 30-second \n",
    "\t          ceilometer data (percent)\n",
    "\t   ACSH = Average cloudiness sunrise to sunset from manual \n",
    "\t          observations (percent)\n",
    "           ADPT = Average Dew Point Temperature for the day (tenths of degrees C)\n",
    "           ASLP = Average Sea Level Pressure for the day (hPa * 10)\n",
    "           ASTP = Average Station Level Pressure for the day (hPa * 10)\n",
    "           AWBT = Average Wet Bulb Temperature for the day (tenths of degrees C)\n",
    "           AWDR = Average daily wind direction (degrees)\n",
    "\t   AWND = Average daily wind speed (tenths of meters per second)\n",
    "\t   DAEV = Number of days included in the multiday evaporation\n",
    "\t          total (MDEV)\n",
    "\t   DAPR = Number of days included in the multiday precipiation \n",
    "\t          total (MDPR)\n",
    "           DASF = Number of days included in the multiday snowfall \n",
    "\t          total (MDSF)\t\t  \n",
    "\t   DATN = Number of days included in the multiday minimum temperature \n",
    "\t         (MDTN)\n",
    "\t   DATX = Number of days included in the multiday maximum temperature \n",
    "\t          (MDTX)\n",
    "           DAWM = Number of days included in the multiday wind movement\n",
    "\t          (MDWM)\n",
    "\t   DWPR = Number of days with non-zero precipitation included in \n",
    "\t          multiday precipitation total (MDPR)\n",
    "\t   EVAP = Evaporation of water from evaporation pan (tenths of mm)\n",
    "\t   FMTM = Time of fastest mile or fastest 1-minute wind \n",
    "\t          (hours and minutes, i.e., HHMM)\n",
    "\t   FRGB = Base of frozen ground layer (cm)\n",
    "\t   FRGT = Top of frozen ground layer (cm)\n",
    "\t   FRTH = Thickness of frozen ground layer (cm)\n",
    "\t   GAHT = Difference between river and gauge height (cm)\n",
    "\t   MDEV = Multiday evaporation total (tenths of mm; use with DAEV)\n",
    "\t   MDPR = Multiday precipitation total (tenths of mm; use with DAPR and \n",
    "\t          DWPR, if available)\n",
    "\t   MDSF = Multiday snowfall total \n",
    "\t   MDTN = Multiday minimum temperature (tenths of degrees C; use with \n",
    "\t          DATN)\n",
    "\t   MDTX = Multiday maximum temperature (tenths of degress C; use with \n",
    "\t          DATX)\n",
    "\t   MDWM = Multiday wind movement (km)\n",
    "           MNPN = Daily minimum temperature of water in an evaporation pan \n",
    "\t         (tenths of degrees C)\n",
    "           MXPN = Daily maximum temperature of water in an evaporation pan \n",
    "\t         (tenths of degrees C)\n",
    "\t   PGTM = Peak gust time (hours and minutes, i.e., HHMM)\n",
    "\t   PSUN = Daily percent of possible sunshine (percent)\n",
    "           RHAV = Average relative humidity for the day (percent)\n",
    "           RHMN = Minimum relative humidity for the day (percent)\n",
    "           RHMX = Maximum relative humidity for the day (percent)\n",
    "\t   SN*# = Minimum soil temperature (tenths of degrees C)\n",
    "\t          where * corresponds to a code\n",
    "\t          for ground cover and # corresponds to a code for soil \n",
    "\t\t  depth.  \n",
    "\t\t  \n",
    "\t\t  Ground cover codes include the following:\n",
    "\t\t  0 = unknown\n",
    "\t\t  1 = grass\n",
    "\t\t  2 = fallow\n",
    "\t\t  3 = bare ground\n",
    "\t\t  4 = brome grass\n",
    "\t\t  5 = sod\n",
    "\t\t  6 = straw multch\n",
    "\t\t  7 = grass muck\n",
    "\t\t  8 = bare muck\n",
    "\t\t  \n",
    "\t\t  Depth codes include the following:\n",
    "\t\t  1 = 5 cm\n",
    "\t\t  2 = 10 cm\n",
    "\t\t  3 = 20 cm\n",
    "\t\t  4 = 50 cm\n",
    "\t\t  5 = 100 cm\n",
    "\t\t  6 = 150 cm\n",
    "\t\t  7 = 180 cm\n",
    "\t\t  \n",
    "\t   SX*# = Maximum soil temperature (tenths of degrees C) \n",
    "\t          where * corresponds to a code for ground cover \n",
    "\t\t  and # corresponds to a code for soil depth. \n",
    "\t\t  See SN*# for ground cover and depth codes. \n",
    "           TAVG = Average temperature (tenths of degrees C)\n",
    "\t          [Note that TAVG from source 'S' corresponds\n",
    "\t\t   to an average for the period ending at\n",
    "\t\t   2400 UTC rather than local midnight]\n",
    "           THIC = Thickness of ice on water (tenths of mm)\t\n",
    " \t   TOBS = Temperature at the time of observation (tenths of degrees C)\n",
    "\t   TSUN = Daily total sunshine (minutes)\n",
    "\t   WDF1 = Direction of fastest 1-minute wind (degrees)\n",
    "\t   WDF2 = Direction of fastest 2-minute wind (degrees)\n",
    "\t   WDF5 = Direction of fastest 5-second wind (degrees)\n",
    "\t   WDFG = Direction of peak wind gust (degrees)\n",
    "\t   WDFI = Direction of highest instantaneous wind (degrees)\n",
    "\t   WDFM = Fastest mile wind direction (degrees)\n",
    "           WDMV = 24-hour wind movement (km)\t   \n",
    "           WESD = Water equivalent of snow on the ground (tenths of mm)\n",
    "\t   WESF = Water equivalent of snowfall (tenths of mm)\n",
    "\t   WSF1 = Fastest 1-minute wind speed (tenths of meters per second)\n",
    "\t   WSF2 = Fastest 2-minute wind speed (tenths of meters per second)\n",
    "\t   WSF5 = Fastest 5-second wind speed (tenths of meters per second)\n",
    "\t   WSFG = Peak gust wind speed (tenths of meters per second)\n",
    "\t   WSFI = Highest instantaneous wind speed (tenths of meters per second)\n",
    "\t   WSFM = Fastest mile wind speed (tenths of meters per second)\n",
    "\t   WT** = Weather Type where ** has one of the following values:\n",
    "\t   \n",
    "                  01 = Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "                  02 = Heavy fog or heaving freezing fog (not always \n",
    "\t\t       distinquished from fog)\n",
    "                  03 = Thunder\n",
    "                  04 = Ice pellets, sleet, snow pellets, or small hail \n",
    "                  05 = Hail (may include small hail)\n",
    "                  06 = Glaze or rime \n",
    "                  07 = Dust, volcanic ash, blowing dust, blowing sand, or \n",
    "\t\t       blowing obstruction\n",
    "                  08 = Smoke or haze \n",
    "                  09 = Blowing or drifting snow\n",
    "                  10 = Tornado, waterspout, or funnel cloud \n",
    "                  11 = High or damaging winds\n",
    "                  12 = Blowing spray\n",
    "                  13 = Mist\n",
    "                  14 = Drizzle\n",
    "                  15 = Freezing drizzle \n",
    "                  16 = Rain (may include freezing rain, drizzle, and\n",
    "\t\t       freezing drizzle) \n",
    "                  17 = Freezing rain \n",
    "                  18 = Snow, snow pellets, snow grains, or ice crystals\n",
    "                  19 = Unknown source of precipitation \n",
    "                  21 = Ground fog \n",
    "                  22 = Ice fog or freezing fog\n",
    "\t\t  \n",
    "            WV** = Weather in the Vicinity where ** has one of the following \n",
    "\t           values:\n",
    "\t\t   \n",
    "\t\t   01 = Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "\t\t   03 = Thunder\n",
    "\t\t   07 = Ash, dust, sand, or other blowing obstruction\n",
    "\t\t   18 = Snow or ice crystals\n",
    "\t\t   20 = Rain or snow shower\n",
    "\t\t   \n",
    "VALUE1     is the value on the first day of the month (missing = -9999).\n",
    "\n",
    "MFLAG1     is the measurement flag for the first day of the month.  There are\n",
    "           ten possible values:\n",
    "\n",
    "           Blank = no measurement information applicable\n",
    "           B     = precipitation total formed from two 12-hour totals\n",
    "           D     = precipitation total formed from four six-hour totals\n",
    "\t   H     = represents highest or lowest hourly temperature (TMAX or TMIN) \n",
    "\t           or the average of hourly values (TAVG)\n",
    "\t   K     = converted from knots \n",
    "\t   L     = temperature appears to be lagged with respect to reported\n",
    "\t           hour of observation \n",
    "           O     = converted from oktas \n",
    "\t   P     = identified as \"missing presumed zero\" in DSI 3200 and 3206\n",
    "           T     = trace of precipitation, snowfall, or snow depth\n",
    "\t   W     = converted from 16-point WBAN code (for wind direction)\n",
    "\n",
    "QFLAG1     is the quality flag for the first day of the month.  There are \n",
    "           fourteen possible values:\n",
    "\n",
    "           Blank = did not fail any quality assurance check\n",
    "           D     = failed duplicate check\n",
    "           G     = failed gap check\n",
    "           I     = failed internal consistency check\n",
    "           K     = failed streak/frequent-value check\n",
    "\t   L     = failed check on length of multiday period \n",
    "           M     = failed megaconsistency check\n",
    "           N     = failed naught check\n",
    "           O     = failed climatological outlier check\n",
    "           R     = failed lagged range check\n",
    "           S     = failed spatial consistency check\n",
    "           T     = failed temporal consistency check\n",
    "           W     = temperature too warm for snow\n",
    "           X     = failed bounds check\n",
    "\t   Z     = flagged as a result of an official Datzilla \n",
    "\t           investigation\n",
    "\n",
    "SFLAG1     is the source flag for the first day of the month.  There are \n",
    "           thirty possible values (including blank, upper and \n",
    "\t   lower case letters):\n",
    "\n",
    "           Blank = No source (i.e., data value missing)\n",
    "           0     = U.S. Cooperative Summary of the Day (NCDC DSI-3200)\n",
    "           6     = CDMP Cooperative Summary of the Day (NCDC DSI-3206)\n",
    "           7     = U.S. Cooperative Summary of the Day -- Transmitted \n",
    "\t           via WxCoder3 (NCDC DSI-3207)\n",
    "           A     = U.S. Automated Surface Observing System (ASOS) \n",
    "                   real-time data (since January 1, 2006)\n",
    "\t   a     = Australian data from the Australian Bureau of Meteorology\n",
    "           B     = U.S. ASOS data for October 2000-December 2005 (NCDC \n",
    "                   DSI-3211)\n",
    "\t   b     = Belarus update\n",
    "\t   C     = Environment Canada\n",
    "\t   D     = Short time delay US National Weather Service CF6 daily \n",
    "\t           summaries provided by the High Plains Regional Climate\n",
    "\t\t   Center\n",
    "\t   E     = European Climate Assessment and Dataset (Klein Tank \n",
    "\t           et al., 2002)\t   \n",
    "           F     = U.S. Fort data \n",
    "           G     = Official Global Climate Observing System (GCOS) or \n",
    "                   other government-supplied data\n",
    "           H     = High Plains Regional Climate Center real-time data\n",
    "           I     = International collection (non U.S. data received through\n",
    "\t           personal contacts)\n",
    "           K     = U.S. Cooperative Summary of the Day data digitized from\n",
    "\t           paper observer forms (from 2011 to present)\n",
    "           M     = Monthly METAR Extract (additional ASOS data)\n",
    "\t   m     = Data from the Mexican National Water Commission (Comision\n",
    "\t           National del Agua -- CONAGUA)\n",
    "\t   N     = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)\n",
    "\t   Q     = Data from several African countries that had been \n",
    "\t           \"quarantined\", that is, withheld from public release\n",
    "\t\t   until permission was granted from the respective \n",
    "\t           meteorological services\n",
    "           R     = NCEI Reference Network Database (Climate Reference Network\n",
    "\t           and Regional Climate Reference Network)\n",
    "\t   r     = All-Russian Research Institute of Hydrometeorological \n",
    "\t           Information-World Data Center\n",
    "           S     = Global Summary of the Day (NCDC DSI-9618)\n",
    "                   NOTE: \"S\" values are derived from hourly synoptic reports\n",
    "                   exchanged on the Global Telecommunications System (GTS).\n",
    "                   Daily values derived in this fashion may differ significantly\n",
    "                   from \"true\" daily data, particularly for precipitation\n",
    "                   (i.e., use with caution).\n",
    "\t   s     = China Meteorological Administration/National Meteorological Information Center/\n",
    "\t           Climatic Data Center (http://cdc.cma.gov.cn)\n",
    "           T     = SNOwpack TELemtry (SNOTEL) data obtained from the U.S. \n",
    "\t           Department of Agriculture's Natural Resources Conservation Service\n",
    "\t   U     = Remote Automatic Weather Station (RAWS) data obtained\n",
    "\t           from the Western Regional Climate Center\t   \n",
    "\t   u     = Ukraine update\t   \n",
    "\t   W     = WBAN/ASOS Summary of the Day from NCDC's Integrated \n",
    "\t           Surface Data (ISD).  \n",
    "           X     = U.S. First-Order Summary of the Day (NCDC DSI-3210)\n",
    "\t   Z     = Datzilla official additions or replacements \n",
    "\t   z     = Uzbekistan update\n",
    "\t   \n",
    "\t   When data are available for the same time from more than one source,\n",
    "\t   the highest priority source is chosen according to the following\n",
    "\t   priority order (from highest to lowest):\n",
    "\t   Z,R,D,0,6,C,X,W,K,7,F,B,M,m,r,E,z,u,b,s,a,G,Q,I,A,N,T,U,H,S\n",
    "\t   \n",
    "\t   \n",
    "VALUE2     is the value on the second day of the month\n",
    "\n",
    "MFLAG2     is the measurement flag for the second day of the month.\n",
    "\n",
    "QFLAG2     is the quality flag for the second day of the month.\n",
    "\n",
    "SFLAG2     is the source flag for the second day of the month.\n",
    "\n",
    "... and so on through the 31st day of the month.  Note: If the month has less \n",
    "than 31 days, then the remaining variables are set to missing (e.g., for April, \n",
    "VALUE31 = -9999, MFLAG31 = blank, QFLAG31 = blank, SFLAG31 = blank).\n",
    "\n",
    "'''\n",
    "print('blank cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65655ab4-dffb-4ccf-b2af-5309421d6332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cell\n"
     ]
    }
   ],
   "source": [
    "# now we have the nearest weather stations for each airport\n",
    "#print(airport_nearest_weather_station['BGM'])\n",
    "# this airport has nearest weather station USC00300684\n",
    "# lets just see what the .dly file looks like\n",
    "#weather_example = spark.read.options().text('./ghcnd_all/USC00300684.dly')\n",
    "#print(weather_example.rdd.map(lambda x:x['value']).map(lambda x: int(x[11:15])).sortBy(lambda x: x).take(1))\n",
    "\n",
    "#weather_example.unpersist()\n",
    "#del(weather_example)\n",
    "\n",
    "print('test cell')\n",
    "\n",
    "# going through each record, finding the origin and destination year and day, and filling in weather info from the .dly files in ghcnd_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033169e8-7837-4ef9-bcb8-94fa45316b0b",
   "metadata": {},
   "source": [
    "**Joining weather data with our original data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711e2362-9daf-4b36-b6d0-effc864b3095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USC00335750', 'USC00415701']\n",
      "1712\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190101...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190102...|\n",
      "|USW00093822190103...|\n",
      "|USW00093822190103...|\n",
      "|USW00093822190103...|\n",
      "|USW00093822190103...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "7380746\n"
     ]
    }
   ],
   "source": [
    "# basically for the values we are interested in, for origin and destination, we add the weather observation to the record\n",
    "# getting the dataframe for only the weather records we might be interested in\n",
    "interested_weather_stations = sc.parallelize(airport_nearest_weather_station.values()).flatMap(lambda x: x).distinct()\n",
    "print(interested_weather_stations.take(2))\n",
    "print(interested_weather_stations.count())\n",
    "\n",
    "# mapping weather stations to their filenames for their weather records\n",
    "interested_weather_stations = interested_weather_stations.map(lambda x: './ghcnd_all/'+x+'.dly')\n",
    "\n",
    "interested_weather_data = spark.read.options().text(interested_weather_stations.collect())\n",
    "\n",
    "interested_weather_stations.unpersist()\n",
    "del(interested_weather_stations)\n",
    "\n",
    "print(interested_weather_data.show())\n",
    "print(interested_weather_data.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247186e-ea9e-4653-ba91-90f7bed38006",
   "metadata": {},
   "source": [
    "**Need to process the weather records to be usable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03caab9e-73c4-45eb-9399-813b746e9957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank cell\n"
     ]
    }
   ],
   "source": [
    " '''The five core elements are (ELEMENTS WE ARE INTERESTED IN FOR THE PROJECT!!!!!):\n",
    "\n",
    "           PRCP = Precipitation (tenths of mm)\n",
    "   \t   SNOW = Snowfall (mm)\n",
    "\t   SNWD = Snow depth (mm)\n",
    "           TMAX = Maximum temperature (tenths of degrees C)\n",
    "           TMIN = Minimum temperature (tenths of degrees C)\n",
    "\t   \n",
    "\t   The other elements are:\n",
    "\t   # Maybe we can take one of these values if they are valid\n",
    "\t   ACMC = Average cloudiness midnight to midnight from 30-second \n",
    "\t          ceilometer data (percent)\n",
    "       ACMH = Average cloudiness midnight to midnight from \n",
    "\t          manual observations (percent)\n",
    "       ACSH = Average cloudiness sunrise to sunset from manual \n",
    "\t          observations (percent)\n",
    "       AWND = Average daily wind speed (tenths of meters per second)\n",
    "       # similarly, pick one of these values if they are valid\n",
    "       WSFM = Fastest mile wind speed (tenths of meters per second)\n",
    "       WSFG = Peak gust wind speed (tenths of meters per second)\n",
    "        \n",
    "'''\n",
    "print('blank cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53d51e0-abb2-4d6b-9377-c99dbb6bf294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete records:0\n",
      "289325\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,collect_list,create_map,col,map_from_arrays\n",
    "\n",
    "#raw.unpersist()\n",
    "#del(raw)\n",
    "# firstly filtering the dataset for elements that we want\n",
    "interested_elements = {'PRCP','SNOW','SNWD','TMAX','TMIN','ACMC','ACMH','ACSH','AWND', 'WSFM', 'WSFG'}\n",
    "#interested_elements = {'PRCP','SNOW','SNWD','TMAX','TMIN','AWND'}\n",
    "def filter_weather(record):\n",
    "    element = record[17:21]\n",
    "    year = record[11:15]\n",
    "    if element in interested_elements and int(year)>=1987:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "num_interested_elements = len(interested_elements)\n",
    "\n",
    "#print(interested_weather_data.rdd.map(lambda x: x['value']).filter(lambda x: x[17:21]=='TMAX' and int(x[11:15])==1987).take(1))\n",
    "interested_weather_data = interested_weather_data.rdd.map(lambda x: x['value']).filter(filter_weather)\n",
    "#print(interested_weather_data.count())\n",
    "\n",
    "# need to process weather records to get basically, month,day,year and values for those days, we do not want -9999 values\n",
    "def process_weather_record(record):\n",
    "    station_id = record[0:11]\n",
    "    year = int(record[11:15])\n",
    "    month = int(record[15:17])\n",
    "    element = record[17:21]\n",
    "    day_values = {}\n",
    "    offset = 21\n",
    "    for i in range(31):\n",
    "        measurement = int(record[offset+(i*8):offset+(i*8)+5])\n",
    "        #print(measurement)\n",
    "        if measurement != -9999:\n",
    "            # we can add it to the dictionary\n",
    "            day_values[i+1] = measurement\n",
    "    return (station_id,year,month,element,day_values)\n",
    "\n",
    "\n",
    "interested_weather_data = interested_weather_data.map(process_weather_record)\n",
    "#print(interested_weather_data.map(lambda x: x[3]).take(1))\n",
    "#print('do we have enough peak wind gust records? ' + str(interested_weather_data.map(lambda x: x[3])\\\n",
    "#                                                         .filter(lambda x: x=='WSFM' or x=='WSFG').count()))\n",
    "#print(interested_weather_data.take(1))\n",
    "\n",
    "#print(interested_weather_data.map(lambda x: ((x[0],x[1],x[2]),(x[3],x[4]))).groupByKey().map(lambda x: (x[0],list(x[1]))).take(1))\n",
    "interested_weather = interested_weather_data.map(lambda x: ((x[0],x[1],x[2]),(x[3],x[4]))).groupByKey().map(lambda x: (x[0],list(x[1])))\n",
    "\n",
    "def process_element_dictionaries(record):\n",
    "    year = record[0][1]\n",
    "    month = record[0][2]\n",
    "    station = record[0][0]\n",
    "    element_dictionary_entries = record[1]\n",
    "    total_measurements={}\n",
    "    for i in range(1,32):\n",
    "        total_measurements[i] = {}\n",
    "        #iterating through days\n",
    "        for element, dictionary in element_dictionary_entries:\n",
    "            if i in dictionary:\n",
    "                # lets add this value to our day cumulative measurements\n",
    "                total_measurements[i][element] = dictionary[i]\n",
    "    return (station,year,month,total_measurements)\n",
    "\n",
    "\n",
    "interested_weather = interested_weather.map(process_element_dictionaries)\n",
    "#test = interested_weather.take(1)\n",
    "#print(test)\n",
    "#print(test[0][3].values())\n",
    "\n",
    "# just seeing how many records contain all the data we are interested in? \n",
    "def complete_records_filter(record):\n",
    "    for dictionary in record[3].values():\n",
    "        if len(dictionary)!=0:\n",
    "            # need this length check for example in 31st day of months with no 31st day\n",
    "            element_set = dictionary.keys()\n",
    "            if len(element_set) != num_interested_elements:\n",
    "                # this stations is missing some features that we want\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "print('complete records:' + str(interested_weather.filter(complete_records_filter).count()))\n",
    "# collecting as map\n",
    "weather_station_data_map = interested_weather.map(lambda x: ((x[0],x[1],x[2]),x[3])).collectAsMap()\n",
    "\n",
    "# now the goal is to accumulate elements for each (year,month,day) tuple for each station\n",
    "'''\n",
    "processed_weather_data = interested_weather_data.toDF(['station','year','month','element','measurements'])\n",
    "\n",
    "processed_weather_data = processed_weather_data.select('station','year','month','element',explode('measurements')\\\n",
    "                                                      .alias('day','measurement'))\\\n",
    "\n",
    "del(interested_weather_data)\n",
    "# need to combine all our elements together into a map\n",
    "processed_weather_data = processed_weather_data.groupBy('station','year','month','day')\\\n",
    "                                               .agg(map_from_arrays(collect_list(processed_weather_data.element),collect_list(processed_weather_data.measurement)))\\\n",
    "                                               .alias('daily_measurements')\n",
    "'''\n",
    "#processed_weather_data = interested_weather.toDF(['station','year','month','daily_measurements'])\n",
    "#processed_weather_data = processed_weather_data.select('station','year','month',explode('daily_measurements')\\\n",
    "                                                      #.alias('day','measurements'))\n",
    "\n",
    "print(len(weather_station_data_map))\n",
    "weather_station_data_map_broadcast = sc.broadcast(weather_station_data_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df858e-73d7-4be2-98bb-615d69f86500",
   "metadata": {},
   "source": [
    "**Joining weather elements to our airline dataset based on year, day, month, and closest station**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea27900-2d5b-4a37-b840-0cb8549232ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank_cell\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Year: integer (nullable = true)\n",
    " |-- Month: integer (nullable = true)\n",
    " |-- DayofMonth: integer (nullable = true)\n",
    " |-- DayOfWeek: integer (nullable = true)\n",
    " |-- DepTime: string (nullable = true)\n",
    " |-- CRSDepTime: integer (nullable = true)\n",
    " |-- ArrTime: string (nullable = true)\n",
    " |-- CRSArrTime: integer (nullable = true)\n",
    " |-- UniqueCarrier: string (nullable = true)\n",
    " |-- FlightNum: integer (nullable = true)\n",
    " |-- TailNum: string (nullable = true)\n",
    " |-- ActualElapsedTime: string (nullable = true)\n",
    " |-- CRSElapsedTime: string (nullable = true)\n",
    " |-- AirTime: string (nullable = true)\n",
    " |-- ArrDelay: string (nullable = true)\n",
    " |-- DepDelay: string (nullable = true)\n",
    " |-- Origin: string (nullable = true)\n",
    " |-- Dest: string (nullable = true)\n",
    " |-- Distance: string (nullable = true)\n",
    " |-- TaxiIn: string (nullable = true)\n",
    " |-- TaxiOut: string (nullable = true)\n",
    " |-- Cancelled: integer (nullable = true)\n",
    " |-- CancellationCode: string (nullable = true)\n",
    " |-- Diverted: integer (nullable = true)\n",
    " |-- CarrierDelay: string (nullable = true)\n",
    " |-- WeatherDelay: string (nullable = true)\n",
    " |-- NASDelay: string (nullable = true)\n",
    " |-- SecurityDelay: string (nullable = true)\n",
    " |-- LateAircraftDelay: string (nullable = true)\n",
    "'''\n",
    "print('blank_cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e542e798-f08e-43e3-bf67-6dc916b3a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+---+\n",
      "|Year|Month|DayofMonth|Airport| id|\n",
      "+----+-----+----------+-------+---+\n",
      "|1993|    1|         5|    TPA|  0|\n",
      "|1993|    1|        22|    MCI|  1|\n",
      "|1993|    1|        17|    OAJ|  2|\n",
      "|1993|    1|        22|    SRQ|  3|\n",
      "|1993|    1|        27|    ELM|  4|\n",
      "|1993|    1|        14|    CAE|  5|\n",
      "|1993|    1|        15|    ABQ|  6|\n",
      "|1993|    1|        24|    MOB|  7|\n",
      "|1993|    1|        30|    ORH|  8|\n",
      "|1993|    1|        26|    SAV|  9|\n",
      "|1993|    1|        18|    MKE| 10|\n",
      "|1993|    1|        11|    MKE| 11|\n",
      "|1993|    1|        27|    IAD| 12|\n",
      "|1993|    1|        15|    BHM| 13|\n",
      "|1993|    1|         1|    SMF| 14|\n",
      "|1993|    1|        26|    SAT| 15|\n",
      "|1993|    1|         5|    ACY| 16|\n",
      "|1993|    1|         9|    TRI| 17|\n",
      "|1993|    1|        17|    AMA| 18|\n",
      "|1993|    1|         4|    OKC| 19|\n",
      "+----+-----+----------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "1708462\n",
      "+---+-----+-----+-----+----+----+----+----+-----+-----+-----+----+\n",
      "| id| ACSH| WSFM| WSFG|SNOW|TMAX|SNWD|PRCP| AWND| ACMH| ACMC|TMIN|\n",
      "+---+-----+-----+-----+----+----+----+----+-----+-----+-----+----+\n",
      "|  0|   90|-9999|   57|   0| 272|   0|   0|   25|   80|-9999| 194|\n",
      "|  1|   30|-9999|  134|   0|  78| 102|   0|   50|   30|-9999| -39|\n",
      "|  2|-9999|-9999|   82|   0|  61|   0| 104|   16|-9999|-9999|   6|\n",
      "|  3|-9999|-9999|-9999|   0| 267|   0|   0|-9999|-9999|-9999| 144|\n",
      "|  4|-9999|-9999|-9999|   0|  11|   0|   0|-9999|-9999|-9999|-106|\n",
      "|  5|   80|-9999|   72|   0| 178|   0|   0|   18|   70|-9999|  44|\n",
      "|  6|   60|-9999|   82|   0| 100|   0|   0|   22|   60|-9999| -17|\n",
      "|  7|  100|-9999|  134|   0| 200|   0| 165|   61|   80|-9999|  56|\n",
      "|  8|   40|-9999|  252|   0| -50|   0|   0|   86|-9999|-9999|-128|\n",
      "|  9|  100|-9999|  118|   0|  78|   0|  64|   65|  100|-9999|  33|\n",
      "| 10|   30|-9999|   67|   0| -67| 127|   0|   30|   30|-9999|-161|\n",
      "| 11|  100|-9999|   77|   8| -22|  76|   5|   47|  100|-9999| -67|\n",
      "| 12|   40|-9999|  139|   0|  72|   0|   0|   36|   20|-9999| -78|\n",
      "| 13|-9999|-9999|-9999|   0|  94|   0|   0|   20|-9999|-9999|  22|\n",
      "| 14|-9999|-9999|-9999|   0| 106|   0| 165|-9999|-9999|-9999|  72|\n",
      "| 15|   30|-9999|   57|   0| 178|   0|   0|   17|   30|-9999| -22|\n",
      "| 16|   80|-9999|  190|   0| 183|   0| 163|   70|   70|-9999|  28|\n",
      "| 17|  100|-9999|   72|   0|  94|   0|   0|   36|  100|-9999|  56|\n",
      "| 18|-9999|-9999|   77|   0| 156|   0|   0|   37|-9999|-9999| -22|\n",
      "| 19|-9999|-9999|  175|   0| 139|   0|  20|   72|-9999|-9999| -44|\n",
      "+---+-----+-----+-----+----+----+----+----+-----+-----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "wrote processed weather dataframe to parquet partitioned by year!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import copy\n",
    "\n",
    "\n",
    "'''\n",
    "IN CASE OOM\n",
    "@F.udf(returnType = T.IntegerType())\n",
    "def day_of_year(year,month,day):\n",
    "    return date(year,month,day).timetuple().tm_yday\n",
    "\n",
    "'''\n",
    "    \n",
    "def add_weather_to_airline_record(row):\n",
    "    year = row['Year']\n",
    "    month = row['Month']\n",
    "    day = row['DayofMonth']\n",
    "    airport = row['Airport']\n",
    "    nearest_stations = airport_nearest_weather_station[airport]\n",
    "    airport_weather = {}\n",
    "    for station in nearest_stations:\n",
    "        if (station,year,month) in weather_station_data_map_broadcast.value:\n",
    "            weather_conditions = weather_station_data_map_broadcast.value[(station,year,month)][day]\n",
    "            for key in weather_conditions:\n",
    "                if key not in airport_weather or airport_weather[key] == -9999:\n",
    "                    airport_weather[key] = weather_conditions[key]\n",
    "    updated_row = row.asDict()\n",
    "    for key in interested_elements:\n",
    "        if key in airport_weather:\n",
    "            updated_row[key] = airport_weather[key]\n",
    "        else:\n",
    "            updated_row[key] = -9999\n",
    "    return Row(**updated_row)\n",
    "\n",
    "def filter_bad_rows(row):\n",
    "    values = row(0)\n",
    "    for val in values:\n",
    "        if type(val)=='int' and val == -9999:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# getting unique airport,year,month,day tuples that we need to map to weather\n",
    "airport_instances = raw.select('Year','Month','DayofMonth',col('Origin').alias('Airport'))\n",
    "dest_airport_instances = raw.select('Year','Month','DayofMonth',col('Dest').alias('Airport'))\n",
    "final_airport_instances = airport_instances.union(dest_airport_instances).distinct().withColumn('id',monotonically_increasing_id())\n",
    "\n",
    "# we can save final_airport instances to parquet\n",
    "print(final_airport_instances.show())\n",
    "print(final_airport_instances.count())\n",
    "\n",
    "def process_only_id(row):\n",
    "    new_dict = {}\n",
    "    new_dict['id'] = row['id']\n",
    "    for key in interested_elements:\n",
    "        new_dict[key] = row[key]\n",
    "    return Row(**new_dict)\n",
    "\n",
    "# there are some airport codes that are in the data, but not in the metadata, we will drop those for sake of processing\n",
    "added_weather_data = final_airport_instances.rdd.filter(lambda x: x['Airport'] in airport_nearest_weather_station)\\\n",
    "                                                .map(add_weather_to_airline_record)\\\n",
    "                                                .map(process_only_id).toDF()\n",
    "\n",
    "\n",
    "print(added_weather_data.show())\n",
    "\n",
    "final_airport_instances.coalesce(1).write.parquet('./airport_date_to_id_mapping')\n",
    "added_weather_data.coalesce(1).write.parquet('./id_to_weather_mapping')\n",
    "\n",
    "\n",
    "# basically, we make a mapping from (airport,year,month,day) to id\n",
    "# then we make a mapping from id to weather data -> thus saving a bit of space\n",
    "# based on the idea that many flights will take off from a given airport in a day and that many will land in a day\n",
    "# so, no need to duplicate the data and join with the original dataset\n",
    "# when we are processing the data in batches, we can just call our lookup then\n",
    "\n",
    "# saving our mapping from (airport,year,month,day) to id in disk\n",
    "# saving our mapping from id to weather in disk\n",
    "\n",
    "\n",
    "\n",
    "print('wrote processed weather dataframe to parquet partitioned by year!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab43ce2a-7c47-4ace-b3d1-817ee56d1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many records have average wind speed != -9999\n",
    "\n",
    "# collecting tuple->id as map\n",
    "#test_tuple_map = final_airport_instances.rdd.map(lambda x: ((x['Year'],x['Month'],x['DayofMonth'],x['Airport']),x['id'])).collectAsMap()\n",
    "\n",
    "#test_id_wather_map = added_weather_data.rdd.map(lambda x: (x['id'],\\\n",
    "#                                                          (x['ACSH'],x['WSFM'],x['WSFG'],x['SNOW'],x['TMAX'],x['SNWD'],\\\n",
    "#                                                          x['PRCP'],x['AWND'],x['ACMH'],x['ACMC'],x['TMIN'])\\\n",
    "#                                                          )).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f411bdf-117d-45a7-bcfc-6198f98d539e",
   "metadata": {},
   "source": [
    "**We have the weather data joined now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004dff8-7f34-4d0a-91fc-53c6527e1b15",
   "metadata": {},
   "source": [
    "**What are the busiest airports that people fly out of?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c49f6d-6bf9-4b36-901b-f0f104aa48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank cell\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# simply counting records for origin airport (doesnt matter if these flights were cancelled or not, its about intent)\n",
    "\n",
    "# need to read in metadata for airport names\n",
    "airport_metadata = spark.read.options(inferSchema=True,header=True).csv('airplane_raw/metadata/airports.csv')\n",
    "iata_airport_pairs = airport_metadata.select('iata', 'airport')\n",
    "origin_top_10 = raw.select('Origin').withColumnRenamed('Origin','iata')\\\n",
    "                                    .groupBy('iata')\\\n",
    "                                    .count()\\\n",
    "                                    .join(iata_airport_pairs,'iata','inner')\\\n",
    "                                    .orderBy('count',ascending=False)\n",
    "origin_top_10.show()\n",
    "'''\n",
    "print('blank cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a42f6-9723-47cd-ad73-ef72a9e45c09",
   "metadata": {},
   "source": [
    "**What are the busiest airports that people fly into?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8393c6-545a-47bf-870a-27d4bc9e7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank cell\n"
     ]
    }
   ],
   "source": [
    "'''# simply counting records for destination airport (doesnt matter if these flights were cancelled or not, its about intent)\n",
    "dest_top_10 = raw.select('Dest').withColumnRenamed('Dest','iata')\\\n",
    "                                    .groupBy('iata')\\\n",
    "                                    .count()\\\n",
    "                                    .join(iata_airport_pairs,'iata','inner')\\\n",
    "                                    .orderBy('count',ascending=False)\n",
    "dest_top_10.show()\n",
    "'''\n",
    "print('blank cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3d52f-b841-4ca5-aec8-d0899c4b875c",
   "metadata": {},
   "source": [
    "**What are the busiest seasons?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "875a1bfa-242e-4473-893f-8d6e88cf1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank cell\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "\n",
    "# September through November is Fall, December through February is Winter, March through May is Spring, June through August is Summer\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@F.udf(returnType=T.StringType())\n",
    "def mapMonthToSeason(month):\n",
    "    if 9<=month<=11:\n",
    "        return \"fall\"\n",
    "    elif month == 12 or month == 1 or month ==2:\n",
    "        return \"winter\"\n",
    "    elif 3<=month<=5:\n",
    "        return \"spring\"\n",
    "    else:\n",
    "        return \"summer\"\n",
    "\n",
    "\n",
    "\n",
    "# mapping flight records to their season\n",
    "season_dataframe = raw.select('Month').withColumn('Month',mapMonthToSeason(F.col('Month'))).withColumnRenamed('Month','Season')\\\n",
    "                                      .groupBy('Season')\\\n",
    "                                      .count()\\\n",
    "                                      .show()'''\n",
    "print('blank cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0be62-cf4e-4036-bad7-2ce664f655aa",
   "metadata": {},
   "source": [
    "**Which airport as the origin has the most delays?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f56c5-417c-49f9-903f-1293730c4e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a5865b-e5a8-4d85-b3eb-26dcb8c0ee8e",
   "metadata": {},
   "source": [
    "**Which airport as the destination has the most delays?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14be6cf-2422-486b-81fa-44c488922950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13cc2d01-d114-43c5-9081-e72dcd600d93",
   "metadata": {},
   "source": [
    "**Which Seasons have the most delays?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca9649-635f-4753-b228-ccad3487f056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2998296a-3978-4f39-99d6-491c2c1b59a8",
   "metadata": {},
   "source": [
    "**Feature Exploration and Distribution Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b744b96-e7da-4bc9-bff8-cd1f76a570e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
