{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a847e5c0-e611-48d1-9655-653d41b08475",
   "metadata": {},
   "source": [
    "**Firstly grabbing our filtered dataset according to mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a883c9e-b23f-4d71-b956-6a626cf48b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading in our raw data\n",
    "raw = spark.read.options(header=True,inferSchema=True).csv('airplane_raw')\n",
    "\n",
    "\n",
    "# only selecting columns which we need\n",
    "# also, we do not want flights which were cancelled (only valid flights or delays)\n",
    "filtered_raw = raw.where(raw.Cancelled == 0).select('Year','Month','DayofMonth','Origin','Dest',\\\n",
    "                                                       'ArrDelay','DepDelay','ActualElapsedTime','Distance')\n",
    "print(raw.printSchema())\n",
    "#filtered_raw = filtered_raw.repartition(15)\n",
    "# Got 116684126 records\n",
    "#print(filtered_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b248ea-84ff-4b73-9708-100ef98167e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- origin_lat: double (nullable = true)\n",
      " |-- origin_long: double (nullable = true)\n",
      " |-- dest_lat: double (nullable = true)\n",
      " |-- dest_long: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# joining latitude and longitude values to the dataset\n",
    "airport_metadata = spark.read.options(header=True,inferSchema=True).csv('./airplane_raw/metadata/airports.csv')\n",
    "print(airport_metadata.printSchema())\n",
    "\n",
    "filtered_raw = filtered_raw.join(airport_metadata.select('iata','lat','long')\\\n",
    "                             .withColumnRenamed('iata','Origin')\\\n",
    "                             .withColumnRenamed('lat','origin_lat')\\\n",
    "                             .withColumnRenamed('long','origin_long'), ['Origin'])\n",
    "\n",
    "filtered_raw = filtered_raw.join(airport_metadata.select('iata','lat','long')\\\n",
    "                             .withColumnRenamed('iata','Dest')\\\n",
    "                             .withColumnRenamed('lat','dest_lat')\\\n",
    "                             .withColumnRenamed('long','dest_long'), ['Dest'])\n",
    "\n",
    "print(filtered_raw.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6fa756-b829-45c9-94b6-7ea4cc732197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- Airport: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ACSH: long (nullable = true)\n",
      " |-- WSFM: long (nullable = true)\n",
      " |-- WSFG: long (nullable = true)\n",
      " |-- SNOW: long (nullable = true)\n",
      " |-- TMAX: long (nullable = true)\n",
      " |-- SNWD: long (nullable = true)\n",
      " |-- PRCP: long (nullable = true)\n",
      " |-- AWND: long (nullable = true)\n",
      " |-- ACMH: long (nullable = true)\n",
      " |-- ACMC: long (nullable = true)\n",
      " |-- TMIN: long (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading or mapping from (year,month,day,airport) to id\n",
    "airport_date_to_id_mapping = spark.read.parquet('./airport_date_to_id_mapping')\n",
    "print(airport_date_to_id_mapping.printSchema())\n",
    "\n",
    "\n",
    "\n",
    "# loading mapping from id to weather conditions\n",
    "id_to_weather_mapping = spark.read.parquet('./id_to_weather_mapping')\n",
    "print(id_to_weather_mapping.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9efcde2-6d28-4e07-a804-9e898a1d4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336772\n"
     ]
    }
   ],
   "source": [
    "# dropping columns in the mapping which we do not need\n",
    "\n",
    "final_weather_mapping = id_to_weather_mapping.select('id','PRCP','TMAX','TMIN','AWND')\\\n",
    "                                             .where(\\\n",
    "                                             (id_to_weather_mapping.PRCP!=-9999) &     \\\n",
    "                                             (id_to_weather_mapping.TMAX!=-9999) & \\\n",
    "                                             (id_to_weather_mapping.TMIN!=-9999) & \\\n",
    "                                             (id_to_weather_mapping.AWND!=-9999) \\\n",
    "                                             )\n",
    "print(final_weather_mapping.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b9b4c3-8183-4768-9aff-2d50db190732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- origin_lat: double (nullable = true)\n",
      " |-- origin_long: double (nullable = true)\n",
      " |-- dest_lat: double (nullable = true)\n",
      " |-- dest_long: double (nullable = true)\n",
      " |-- origin_id: long (nullable = true)\n",
      " |-- dest_id: long (nullable = true)\n",
      " |-- Season: integer (nullable = true)\n",
      "\n",
      "None\n",
      "[Row(ArrDelay='16', DepDelay='0', ActualElapsedTime='93', Distance='282', origin_lat=40.69249722, origin_long=-74.16866056, dest_lat=42.94052472, dest_long=-78.73216667, origin_id=85899346091, dest_id=146028888418, Season=0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# mapping location and day,time to ids in the raw df\n",
    "filtered_raw = filtered_raw.join(airport_date_to_id_mapping.withColumnRenamed('Airport','Origin').withColumnRenamed('id','origin_id'), \\\n",
    "                                ['Year','Month','DayofMonth','Origin'])\n",
    "filtered_raw = filtered_raw.join(airport_date_to_id_mapping.withColumnRenamed('Airport','Dest').withColumnRenamed('id','dest_id'), \\\n",
    "                                ['Year','Month','DayofMonth','Dest'])\n",
    "\n",
    "# adding a dayOfYear column\n",
    "@F.udf(returnType = T.StringType())\n",
    "def date_string(year,month,day):\n",
    "    return year+'-'+month+'-'+day\n",
    "\n",
    "# 12-2 is winter =0, 3-5 is spring=1, 6-8 is summer=2, 9-11 is fall=3\n",
    "@F.udf(returnType = T.IntegerType())\n",
    "def map_month_to_season(month):\n",
    "    if month in {12,1,2}:\n",
    "        return 0\n",
    "    elif month in {3,4,5}:\n",
    "        return 1\n",
    "    elif month in {6,7,8}:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# dropping the year, month, dayofMonth, Origin,and Dest columns, since we have their unique ids\n",
    "#filtered_raw = filtered_raw.withColumn('Date',F.concat_ws(\"-\",filtered_raw.Year,filtered_raw.Month,filtered_raw.DayofMonth).cast(\"date\"))\n",
    "#filtered_raw = filtered_raw.withColumn('Date', F.dayofyear(filtered_raw.Date)).withColumnRenamed('Date','DayOfYear')\n",
    "filtered_raw = filtered_raw.withColumn('Season',map_month_to_season(filtered_raw.Month))\n",
    "filtered_raw = filtered_raw.drop('Year','Month','DayofMonth','Origin','Dest')\n",
    "print(filtered_raw.printSchema())\n",
    "print(filtered_raw.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f218b618-4778-4e86-9a39-2b843e8b0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "# checking exactly how many records with weather that we have\n",
    "\n",
    "# below collects map of (id -> [Weather conditions])\n",
    "collect_weather = final_weather_mapping.rdd.map(lambda x: (x['id'],(x['PRCP'],x['TMAX'],x['TMIN'],x['AWND']))).collectAsMap()\n",
    "\n",
    "collect_weather_broadcast = sc.broadcast(collect_weather)\n",
    "\n",
    "# basically just grabbing records which are valid\n",
    "def test_good_records(record):\n",
    "    if record['ArrDelay'] == 'NA':\n",
    "        return False\n",
    "    elif record['DepDelay'] == 'NA':\n",
    "        return False\n",
    "    elif record['ActualElapsedTime'] == 'NA':\n",
    "        return False\n",
    "    elif record['Distance']=='NA':\n",
    "        return False\n",
    "    elif isnan(record['origin_lat']):\n",
    "        return False\n",
    "    elif isnan(record['origin_long']):\n",
    "        return False\n",
    "    elif isnan(record['dest_lat']):\n",
    "        return False\n",
    "    elif isnan(record['dest_long']):\n",
    "        return False\n",
    "    elif record['Season']==None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "filtered_raw = \\\n",
    "                filtered_raw.rdd\\\n",
    "                             .filter(test_good_records)\\\n",
    "                             .filter(lambda x: x['origin_id'] in collect_weather_broadcast.value \\\n",
    "                             and x['dest_id'] in collect_weather_broadcast.value).toDF()\n",
    "\n",
    "#print(filtered_raw.where(filtered_raw.ArrDelay == 'NA').count())\n",
    "#print(filtered_raw.where(filtered_raw.ActualElapsedTime == 'NA').count())\n",
    "# 95830696\n",
    "#print(filtered_raw.count())\n",
    "# turning string columns into ints, so we can run BFR\n",
    "\n",
    "filtered_raw = filtered_raw\\\n",
    "                           .withColumn('ArrDelay',filtered_raw.ArrDelay.cast('int'))\\\n",
    "                           .withColumn('DepDelay',filtered_raw.DepDelay.cast('int'))\\\n",
    "                           .withColumn('ActualElapsedTime',filtered_raw.ActualElapsedTime.cast('int'))\\\n",
    "                           .withColumn('Distance',filtered_raw.Distance.cast('int'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6627fac4-9e51-4e78-916f-3df0e6a8679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ArrDelay_mean=7.159954113241544, DepDelay_mean=8.155785553305384, ActualElapsedTime_mean=122.11903171401364, Distance_mean=715.1781956065518, origin_lat_mean=37.099659857405534, origin_long_mean=-92.27161607308402, dest_lat_mean=37.099157241985075, dest_long_mean=-92.26712071709842, Season_mean=1.4983448205364176, origin_prcp_mean=25.696228836739326, origin_tmax_mean=205.3709193033514, origin_tmin_mean=94.5765910747429, origin_awnd_mean=38.636158741871185, dest_prcp_mean=25.612486723460716, dest_tmax_mean=205.36587148443542, dest_tmin_mean=94.56188964755093, dest_awnd_mean=38.63819560488218, ArrDelay_stddev=30.939682127010062, DepDelay_stddev=28.437824166810316, ActualElapsedTime_stddev=70.31234179026455, Distance_stddev=569.0274209367313, origin_lat_stddev=5.599107569908229, origin_long_stddev=16.703784257869227, dest_lat_stddev=5.59970726731377, dest_long_stddev=16.697214735116198, Season_stddev=1.1130543679507285, origin_prcp_stddev=83.8530474281069, origin_tmax_stddev=105.31647510766034, origin_tmin_stddev=98.13591478796418, origin_awnd_stddev=20.65944904665206, dest_prcp_stddev=83.51747812154095, dest_tmax_stddev=105.35316315020057, dest_tmin_stddev=98.17185756564172, dest_awnd_stddev=20.57745406162838)\n",
      "[Row(ArrDelay=0.2857187042345589, DepDelay=-0.2867935853835109, ActualElapsedTime=-0.4141382717826853, Distance=-0.7612606698170274, origin_lat=0.6416803602602251, origin_long=1.083763728842201, dest_lat=1.043155864970256, dest_long=0.8106114859164403, Season=0, origin_prcp=-0.30644358940884653, origin_tmax=-1.313858246413023, origin_tmin=-1.4732281386189758, origin_awnd=1.9053674262677334, dest_prcp=-0.18693675952164462, dest_tmax=-1.8923577187730207, dest_tmin=-1.8698015317150736, dest_awnd=2.83620141832548)]\n"
     ]
    }
   ],
   "source": [
    "# need to grab mean and std for each column so that we can normalize the df and apply clustering\n",
    "\n",
    "\n",
    "joined = filtered_raw.join(final_weather_mapping.withColumnRenamed('id','origin_id'),'origin_id')\\\n",
    "                     .withColumnRenamed('PRCP','origin_prcp')\\\n",
    "                     .withColumnRenamed('TMAX','origin_tmax')\\\n",
    "                     .withColumnRenamed('TMIN','origin_tmin')\\\n",
    "                     .withColumnRenamed('AWND','origin_awnd')\\\n",
    "                     .join(final_weather_mapping.withColumnRenamed('id','dest_id'),'dest_id')\\\n",
    "                     .withColumnRenamed('PRCP','dest_prcp')\\\n",
    "                     .withColumnRenamed('TMAX','dest_tmax')\\\n",
    "                     .withColumnRenamed('TMIN','dest_tmin')\\\n",
    "                     .withColumnRenamed('AWND','dest_awnd')\\\n",
    "                     .drop('dest_id','origin_id')\n",
    "\n",
    "columns = joined.columns\n",
    "#print(joined.printSchema())\n",
    "\n",
    "# below is for mean, standard deviation based scaling\n",
    "\n",
    "stats = joined.select(*[[F.mean(c).alias(c+'_mean') for c in joined.columns] + [F.stddev_pop(c).alias(c+'_stddev') for c in joined.columns]]).collect()[0]\n",
    "# scaling join column\n",
    "for column in columns:\n",
    "    if column != 'Season':\n",
    "        joined = joined.withColumn(column,(joined[column]-stats[column+'_mean'])/(stats[column+'_stddev']))\n",
    "\n",
    "# below is for max, min based scaling\n",
    "'''stats = joined.select(*[[F.max(c).alias(c+'_max') for c in joined.columns] + [F.min(c).alias(c+'_min') for c in joined.columns]]).collect()[0]\n",
    "# scaling join column\n",
    "for column in columns:\n",
    "    if column != 'Season':\n",
    "        joined = joined.withColumn(column,(joined[column]-stats[column+'_min'])/(stats[column+'_max'] - stats[column+'_min']))'''\n",
    "\n",
    "print(stats)\n",
    "\n",
    "\n",
    "print(joined.take(1))\n",
    "\n",
    "# lets write this data to parquet, just because\n",
    "#joined.write.parquet('./FINAL_processed_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1882822-1c17-4549-a2c9-ecf5e94d21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- origin_lat: double (nullable = true)\n",
      " |-- origin_long: double (nullable = true)\n",
      " |-- dest_lat: double (nullable = true)\n",
      " |-- dest_long: double (nullable = true)\n",
      " |-- Season: long (nullable = true)\n",
      " |-- origin_prcp: double (nullable = true)\n",
      " |-- origin_tmax: double (nullable = true)\n",
      " |-- origin_tmin: double (nullable = true)\n",
      " |-- origin_awnd: double (nullable = true)\n",
      " |-- dest_prcp: double (nullable = true)\n",
      " |-- dest_tmax: double (nullable = true)\n",
      " |-- dest_tmin: double (nullable = true)\n",
      " |-- dest_awnd: double (nullable = true)\n",
      "\n",
      "None\n",
      "Season = 1, K = 3, Error = 315711639.12549585\n",
      "Season = 1, K = 4, Error = 281390076.69847995\n",
      "Season = 1, K = 5, Error = 272308298.4434357\n",
      "Season = 1, K = 6, Error = 250646340.6033714\n",
      "Season = 1, K = 7, Error = 257203633.17563495\n",
      "Season = 1, K = 8, Error = 226601185.17027733\n",
      "Season = 1, K = 9, Error = 219838515.9333366\n",
      "Season = 2, K = 3, Error = 345471073.78736234\n",
      "Season = 2, K = 4, Error = 314733763.0316134\n",
      "Season = 2, K = 5, Error = 322933422.09953505\n",
      "Season = 2, K = 6, Error = 279052031.76005566\n",
      "Season = 2, K = 7, Error = 275461092.1829175\n",
      "Season = 2, K = 8, Error = 249967532.08822915\n",
      "Season = 2, K = 9, Error = 242998230.16929585\n",
      "Season = 3, K = 3, Error = 319624462.79450023\n",
      "Season = 3, K = 4, Error = 295372542.9395626\n",
      "Season = 3, K = 5, Error = 276603175.7652543\n",
      "Season = 3, K = 6, Error = 259550973.15776765\n",
      "Season = 3, K = 7, Error = 241416053.89247972\n",
      "Season = 3, K = 8, Error = 261095330.90458247\n",
      "Season = 3, K = 9, Error = 225039990.62962836\n",
      "Season = 4, K = 3, Error = 1479235977.3868341\n",
      "Season = 4, K = 4, Error = 1324896523.6018512\n",
      "Season = 4, K = 5, Error = 1425125145.3080542\n",
      "Season = 4, K = 6, Error = 1201994353.7041962\n",
      "Season = 4, K = 7, Error = 1192265689.0606742\n",
      "Season = 4, K = 8, Error = 1104988826.240908\n",
      "Season = 4, K = 9, Error = 1121324086.4597251\n"
     ]
    }
   ],
   "source": [
    "# bfr loop\n",
    "from BFR_Logic import BFR\n",
    "from math import sqrt\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# adding in weather data based on id\n",
    "def add_weather(row):\n",
    "    origin_weather = collect_weather_broadcast.value[row['origin_id']]\n",
    "    dest_weather = collect_weather_broadcast.value[row['dest_id']]\n",
    "    updated_row = row.asDict()\n",
    "    updated_row['origin_prcp'] = origin_weather[0]\n",
    "    updated_row['origin_tmax'] = origin_weather[1]\n",
    "    updated_row['origin_tmin'] = origin_weather[2]\n",
    "    updated_row['origin_awnd'] = origin_weather[3]\n",
    "    \n",
    "    updated_row['dest_prcp'] = dest_weather[0]\n",
    "    updated_row['dest_tmax'] = dest_weather[1]\n",
    "    updated_row['dest_tmin'] = dest_weather[2]\n",
    "    updated_row['dest_awnd'] = dest_weather[3]\n",
    "    \n",
    "    del(updated_row['origin_id'])\n",
    "    del(updated_row['dest_id'])\n",
    "    \n",
    "    return Row(**updated_row)\n",
    "\n",
    "def map_row_to_list(row):\n",
    "    return (\\\n",
    "            row['CRSDepTime'],\\\n",
    "            row['ArrDelay'],\\\n",
    "            row['DepDelay'],\\\n",
    "            row['ActualElapsedTime'],\\\n",
    "            row['Distance'],\\\n",
    "            row['origin_lat'],\\\n",
    "            row['origin_long'],\\\n",
    "            row['dest_lat'],\\\n",
    "            row['dest_long'],\\\n",
    "            row['DayOfYear'],\\\n",
    "            row['origin_prcp']\\\n",
    "           )\n",
    "\n",
    "def squared_error(centers, x):\n",
    "    dist = np.zeros(len(centers))\n",
    "    vals = []\n",
    "    for val in x:\n",
    "        vals.append(val)\n",
    "    vals = np.array(vals)\n",
    "    for c in range(len(centers)):\n",
    "        dist[c] = np.linalg.norm(vals-centers[c][0])\n",
    "    return dist[np.argmin(dist)]**2\n",
    "    \n",
    "# num features\n",
    "num_features = len(joined.columns)\n",
    "\n",
    "print(joined.printSchema())\n",
    "\n",
    "# num_clusters\n",
    "for season in range(5):\n",
    "    # season==4 means we take EVERYTHING\n",
    "    if season ==0 :\n",
    "        continue\n",
    "    errors = np.zeros(7)\n",
    "    for k in range(3,10):\n",
    "        \n",
    "        # getting centroids\n",
    "        if season!=4:\n",
    "            centroids = spark.read.parquet('./BFR_Clusters/k='+str(k)+'_season='+str(season)).select('centroid').toPandas().to_numpy()\n",
    "        else:\n",
    "            centroids = spark.read.parquet('./BFR_Clusters/k='+str(k)+'_season=EVERYTHING').select('centroid').toPandas().to_numpy()\n",
    "        \n",
    "        if season!=4:\n",
    "            final_dat = joined.where(joined.Season == season).drop('Season')\n",
    "        else:\n",
    "            final_dat = joined.drop('Season')\n",
    "\n",
    "        error = final_dat.rdd.map(lambda x: squared_error(centroids,x)).reduce(lambda x,y: x+y)\n",
    " \n",
    "        errors[k-3]=error\n",
    "        print('Season = '+str(season)+', K = '+str(k)+', Error = '+str(errors[k-3]))\n",
    " \n",
    "    # writing errors to disk\n",
    "    spark_errors = [(k+3,error.item()) for k,error in enumerate(errors)]\n",
    "    if season!=4:\n",
    "        sc.parallelize(spark_errors).toDF(['k','error']).coalesce(1).write.option('header',True)\\\n",
    "                                    .csv('./BFR_Errors/season='+str(season))\n",
    "    else:\n",
    "        sc.parallelize(spark_errors).toDF(['k','error']).coalesce(1).write.option('header',True)\\\n",
    "                                    .csv('./BFR_Errors/season=EVERYTHING')\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c11ce-e17d-4548-a277-a449d571e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
