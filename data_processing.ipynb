{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a847e5c0-e611-48d1-9655-653d41b08475",
   "metadata": {},
   "source": [
    "**Firstly grabbing our filtered dataset according to mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a883c9e-b23f-4d71-b956-6a626cf48b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading in our raw data\n",
    "raw = spark.read.options(header=True,inferSchema=True).csv('airplane_raw')\n",
    "\n",
    "\n",
    "# only selecting columns which we need\n",
    "# also, we do not want flights which were cancelled (only valid flights or delays)\n",
    "filtered_raw = raw.where(raw.Cancelled == 0).select('Year','Month','DayofMonth','Origin','Dest',\\\n",
    "                                                       'ArrDelay','DepDelay','ActualElapsedTime','Distance')\n",
    "print(raw.printSchema())\n",
    "#filtered_raw = filtered_raw.repartition(15)\n",
    "# Got 116684126 records\n",
    "#print(filtered_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b248ea-84ff-4b73-9708-100ef98167e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- origin_lat: double (nullable = true)\n",
      " |-- origin_long: double (nullable = true)\n",
      " |-- dest_lat: double (nullable = true)\n",
      " |-- dest_long: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# joining latitude and longitude values to the dataset\n",
    "airport_metadata = spark.read.options(header=True,inferSchema=True).csv('./airplane_raw/metadata/airports.csv')\n",
    "print(airport_metadata.printSchema())\n",
    "\n",
    "filtered_raw = filtered_raw.join(airport_metadata.select('iata','lat','long')\\\n",
    "                             .withColumnRenamed('iata','Origin')\\\n",
    "                             .withColumnRenamed('lat','origin_lat')\\\n",
    "                             .withColumnRenamed('long','origin_long'), ['Origin'])\n",
    "\n",
    "filtered_raw = filtered_raw.join(airport_metadata.select('iata','lat','long')\\\n",
    "                             .withColumnRenamed('iata','Dest')\\\n",
    "                             .withColumnRenamed('lat','dest_lat')\\\n",
    "                             .withColumnRenamed('long','dest_long'), ['Dest'])\n",
    "\n",
    "print(filtered_raw.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6fa756-b829-45c9-94b6-7ea4cc732197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- Airport: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ACSH: long (nullable = true)\n",
      " |-- WSFM: long (nullable = true)\n",
      " |-- WSFG: long (nullable = true)\n",
      " |-- SNOW: long (nullable = true)\n",
      " |-- TMAX: long (nullable = true)\n",
      " |-- SNWD: long (nullable = true)\n",
      " |-- PRCP: long (nullable = true)\n",
      " |-- AWND: long (nullable = true)\n",
      " |-- ACMH: long (nullable = true)\n",
      " |-- ACMC: long (nullable = true)\n",
      " |-- TMIN: long (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading or mapping from (year,month,day,airport) to id\n",
    "airport_date_to_id_mapping = spark.read.parquet('./airport_date_to_id_mapping')\n",
    "print(airport_date_to_id_mapping.printSchema())\n",
    "\n",
    "\n",
    "\n",
    "# loading mapping from id to weather conditions\n",
    "id_to_weather_mapping = spark.read.parquet('./id_to_weather_mapping')\n",
    "print(id_to_weather_mapping.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9efcde2-6d28-4e07-a804-9e898a1d4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336772\n"
     ]
    }
   ],
   "source": [
    "# dropping columns in the mapping which we do not need\n",
    "\n",
    "final_weather_mapping = id_to_weather_mapping.select('id','PRCP','TMAX','TMIN','AWND')\\\n",
    "                                             .where(\\\n",
    "                                             (id_to_weather_mapping.PRCP!=-9999) &     \\\n",
    "                                             (id_to_weather_mapping.TMAX!=-9999) & \\\n",
    "                                             (id_to_weather_mapping.TMIN!=-9999) & \\\n",
    "                                             (id_to_weather_mapping.AWND!=-9999) \\\n",
    "                                             )\n",
    "print(final_weather_mapping.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b9b4c3-8183-4768-9aff-2d50db190732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- origin_lat: double (nullable = true)\n",
      " |-- origin_long: double (nullable = true)\n",
      " |-- dest_lat: double (nullable = true)\n",
      " |-- dest_long: double (nullable = true)\n",
      " |-- origin_id: long (nullable = true)\n",
      " |-- dest_id: long (nullable = true)\n",
      " |-- Season: integer (nullable = true)\n",
      "\n",
      "None\n",
      "[Row(ArrDelay='16', DepDelay='0', ActualElapsedTime='93', Distance='282', origin_lat=40.69249722, origin_long=-74.16866056, dest_lat=42.94052472, dest_long=-78.73216667, origin_id=85899346091, dest_id=146028888418, Season=0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# mapping location and day,time to ids in the raw df\n",
    "filtered_raw = filtered_raw.join(airport_date_to_id_mapping.withColumnRenamed('Airport','Origin').withColumnRenamed('id','origin_id'), \\\n",
    "                                ['Year','Month','DayofMonth','Origin'])\n",
    "filtered_raw = filtered_raw.join(airport_date_to_id_mapping.withColumnRenamed('Airport','Dest').withColumnRenamed('id','dest_id'), \\\n",
    "                                ['Year','Month','DayofMonth','Dest'])\n",
    "\n",
    "# adding a dayOfYear column\n",
    "@F.udf(returnType = T.StringType())\n",
    "def date_string(year,month,day):\n",
    "    return year+'-'+month+'-'+day\n",
    "\n",
    "# 12-2 is winter =0, 3-5 is spring=1, 6-8 is summer=2, 9-11 is fall=3\n",
    "@F.udf(returnType = T.IntegerType())\n",
    "def map_month_to_season(month):\n",
    "    if month in {12,1,2}:\n",
    "        return 0\n",
    "    elif month in {3,4,5}:\n",
    "        return 1\n",
    "    elif month in {6,7,8}:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# dropping the year, month, dayofMonth, Origin,and Dest columns, since we have their unique ids\n",
    "#filtered_raw = filtered_raw.withColumn('Date',F.concat_ws(\"-\",filtered_raw.Year,filtered_raw.Month,filtered_raw.DayofMonth).cast(\"date\"))\n",
    "#filtered_raw = filtered_raw.withColumn('Date', F.dayofyear(filtered_raw.Date)).withColumnRenamed('Date','DayOfYear')\n",
    "filtered_raw = filtered_raw.withColumn('Season',map_month_to_season(filtered_raw.Month))\n",
    "filtered_raw = filtered_raw.drop('Year','Month','DayofMonth','Origin','Dest')\n",
    "print(filtered_raw.printSchema())\n",
    "print(filtered_raw.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f218b618-4778-4e86-9a39-2b843e8b0fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95830696\n"
     ]
    }
   ],
   "source": [
    "from math import isnan\n",
    "# checking exactly how many records with weather that we have\n",
    "\n",
    "# below collects map of (id -> [Weather conditions])\n",
    "collect_weather = final_weather_mapping.rdd.map(lambda x: (x['id'],(x['PRCP'],x['TMAX'],x['TMIN'],x['AWND']))).collectAsMap()\n",
    "\n",
    "collect_weather_broadcast = sc.broadcast(collect_weather)\n",
    "\n",
    "# basically just grabbing records which are valid\n",
    "def test_good_records(record):\n",
    "    if record['ArrDelay'] == 'NA':\n",
    "        return False\n",
    "    elif record['DepDelay'] == 'NA':\n",
    "        return False\n",
    "    elif record['ActualElapsedTime'] == 'NA':\n",
    "        return False\n",
    "    elif record['Distance']=='NA':\n",
    "        return False\n",
    "    elif isnan(record['origin_lat']):\n",
    "        return False\n",
    "    elif isnan(record['origin_long']):\n",
    "        return False\n",
    "    elif isnan(record['dest_lat']):\n",
    "        return False\n",
    "    elif isnan(record['dest_long']):\n",
    "        return False\n",
    "    elif record['Season']==None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "filtered_raw = \\\n",
    "                filtered_raw.rdd\\\n",
    "                             .filter(test_good_records)\\\n",
    "                             .filter(lambda x: x['origin_id'] in collect_weather_broadcast.value \\\n",
    "                             and x['dest_id'] in collect_weather_broadcast.value).toDF()\n",
    "\n",
    "#print(filtered_raw.where(filtered_raw.ArrDelay == 'NA').count())\n",
    "#print(filtered_raw.where(filtered_raw.ActualElapsedTime == 'NA').count())\n",
    "# 95830696\n",
    "print(filtered_raw.count())\n",
    "# turning string columns into ints, so we can run BFR\n",
    "\n",
    "filtered_raw = filtered_raw\\\n",
    "                           .withColumn('ArrDelay',filtered_raw.ArrDelay.cast('int'))\\\n",
    "                           .withColumn('DepDelay',filtered_raw.DepDelay.cast('int'))\\\n",
    "                           .withColumn('ActualElapsedTime',filtered_raw.ActualElapsedTime.cast('int'))\\\n",
    "                           .withColumn('Distance',filtered_raw.Distance.cast('int'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6627fac4-9e51-4e78-916f-3df0e6a8679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- origin_lat: double (nullable = true)\n",
      " |-- origin_long: double (nullable = true)\n",
      " |-- dest_lat: double (nullable = true)\n",
      " |-- dest_long: double (nullable = true)\n",
      " |-- Season: long (nullable = true)\n",
      " |-- origin_prcp: long (nullable = true)\n",
      " |-- origin_tmax: long (nullable = true)\n",
      " |-- origin_tmin: long (nullable = true)\n",
      " |-- origin_awnd: long (nullable = true)\n",
      " |-- dest_prcp: long (nullable = true)\n",
      " |-- dest_tmax: long (nullable = true)\n",
      " |-- dest_tmin: long (nullable = true)\n",
      " |-- dest_awnd: long (nullable = true)\n",
      " |-- record_id: long (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# need to grab mean and std for each column so that we can normalize the df and apply clustering\n",
    "\n",
    "\n",
    "joined = filtered_raw.join(final_weather_mapping.withColumnRenamed('id','origin_id'),'origin_id')\\\n",
    "                     .withColumnRenamed('PRCP','origin_prcp')\\\n",
    "                     .withColumnRenamed('TMAX','origin_tmax')\\\n",
    "                     .withColumnRenamed('TMIN','origin_tmin')\\\n",
    "                     .withColumnRenamed('AWND','origin_awnd')\\\n",
    "                     .join(final_weather_mapping.withColumnRenamed('id','dest_id'),'dest_id')\\\n",
    "                     .withColumnRenamed('PRCP','dest_prcp')\\\n",
    "                     .withColumnRenamed('TMAX','dest_tmax')\\\n",
    "                     .withColumnRenamed('TMIN','dest_tmin')\\\n",
    "                     .withColumnRenamed('AWND','dest_awnd')\\\n",
    "                     .drop('dest_id','origin_id')\n",
    "\n",
    "# finally, adding a record id to the dataframe (we will need this for BFR since records are summarized in batches)\n",
    "joined = joined.withColumn('record_id',F.monotonically_increasing_id())\n",
    "print(joined.printSchema())\n",
    "# lets write this data to parquet\n",
    "joined.write.parquet('./FINAL_processed_data')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
